# syntax=docker/dockerfile:1
# SPDX-License-Identifier: MIT
# SPDX-FileCopyrightText: 2025 nop
#
# Remote VLA-RL Dockerfile for CRANE-X7
# Build context: project root (crane_x7_vla/)

ARG CUDA_VERSION=12.8.0

# Base Stage: CUDA + Python + Tailscale + SSH
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-ubuntu22.04 AS base

LABEL maintainer="nop" \
    org.opencontainers.image.source="https://github.com/NOPLAB/crane_x7_vla" \
    org.opencontainers.image.description="Remote VLA-RL training environment for CRANE-X7 with Tailscale/SSH" \
    org.opencontainers.image.licenses="MIT"

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda

# Install system dependencies, Tailscale, and SSH server
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    # Basic tools
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
    git \
    vim \
    tmux \
    htop \
    # Python
    python3-pip \
    python3-dev \
    # Libraries
    build-essential \
    libgomp1 \
    libglib2.0-0 \
    # EGL/OpenGL for headless rendering
    libegl1 \
    libegl1-mesa-dev \
    libgl1 \
    libgl1-mesa-glx \
    libglu1-mesa \
    libopengl0 \
    libglx0 \
    libgles2 \
    libglvnd-dev \
    libegl-mesa0 \
    # X11 for GUI forwarding
    xauth \
    x11-apps \
    libx11-6 \
    libxext6 \
    libxrender1 \
    libxtst6 \
    libxi6 \
    # Network tools
    iproute2 \
    iputils-ping \
    socat \
    # SSH server
    openssh-server \
    && mkdir -p /run/sshd \
    # Install Tailscale
    && curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg \
       | tee /usr/share/keyrings/tailscale-archive-keyring.gpg >/dev/null \
    && curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list \
       | tee /etc/apt/sources.list.d/tailscale.list \
    && apt-get update && apt-get install -y --no-install-recommends tailscale

# Configure SSH for X11 forwarding
RUN sed -i 's/#X11Forwarding yes/X11Forwarding yes/' /etc/ssh/sshd_config \
    && sed -i 's/#X11UseLocalhost yes/X11UseLocalhost no/' /etc/ssh/sshd_config \
    && sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin no/' /etc/ssh/sshd_config \
    && sed -i 's/#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config \
    && echo "AllowAgentForwarding yes" >> /etc/ssh/sshd_config \
    && echo "AllowTcpForwarding yes" >> /etc/ssh/sshd_config

# Create non-root user for SSH
RUN useradd -m -s /bin/bash -G sudo vla \
    && mkdir -p /home/vla/.ssh \
    && chmod 700 /home/vla/.ssh \
    && chown -R vla:vla /home/vla

# Upgrade pip
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    python3 -m pip install --upgrade pip "setuptools<78" wheel "packaging<24"

WORKDIR /workspace

# Configure NVIDIA for headless rendering
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=graphics,utility,compute

# Install PyTorch with CUDA support (cu128 for CUDA 12.8)
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Install VLA dependencies
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install \
    transformers \
    accelerate \
    "timm>=0.9.10,<1.0.0" \
    peft \
    huggingface_hub \
    pillow \
    numpy \
    sentencepiece \
    tokenizers \
    einops \
    draccus

# Install VLA-RL and simulator dependencies
COPY vla-rl/requirements.txt /build/requirements-vla-rl.txt
COPY sim/requirements.txt /build/requirements-sim.txt

RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install -r /build/requirements-vla-rl.txt -r /build/requirements-sim.txt

# Copy OpenVLA/prismatic source
COPY vla/src/openvla /workspace/vla/src/openvla

# Copy and install lift simulator
COPY --chown=root:root sim /workspace/sim
WORKDIR /workspace/sim
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install --no-deps .

# Copy and install vla-rl package
COPY --chown=root:root vla-rl /workspace/vla-rl
WORKDIR /workspace/vla-rl
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install --no-deps .

# Create directories
RUN mkdir -p /workspace/vla-rl/outputs \
    /workspace/data \
    /workspace/models \
    /var/lib/tailscale

# Give vla user access to workspace
RUN chown -R vla:vla /workspace

# Copy entrypoint
COPY docker/entrypoint-remote-vla-rl.sh /entrypoint.sh
COPY docker/wait-for-peer.sh /usr/local/bin/wait-for-peer.sh
RUN chmod +x /entrypoint.sh /usr/local/bin/wait-for-peer.sh

# Environment variables
ENV PYTHONPATH="/workspace/vla/src/openvla:/workspace/vla-rl/src:/workspace/sim/src:/workspace"

# CUDA/PyTorch optimization
ENV CUDA_LAUNCH_BLOCKING=0 \
    PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512" \
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 \
    TF_CPP_MIN_LOG_LEVEL=2

# W&B
ENV WANDB_DIR=/workspace/vla-rl/outputs

# Tailscale configuration (override at runtime)
ENV TS_AUTHKEY="" \
    TS_HOSTNAME="crane-x7-vla-rl" \
    TS_STATE_DIR="/var/lib/tailscale" \
    TS_USERSPACE="true"

# SSH configuration (override at runtime)
# Provide public key via SSH_PUBLIC_KEY environment variable
ENV SSH_PUBLIC_KEY="" \
    SSH_PORT="22"

# VLA-RL configuration (override at runtime)
ENV VLA_MODEL_PATH="" \
    HF_TOKEN="" \
    HF_HOME="/root/.cache/huggingface"

WORKDIR /workspace

EXPOSE 22

ENTRYPOINT ["/entrypoint.sh"]
CMD ["sshd"]


# Development Stage: Additional dev tools
FROM base AS dev

RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    pip3 install \
        pytest \
        pytest-cov \
        ipython \
        jupyter \
        black \
        ruff

CMD ["/bin/bash"]
